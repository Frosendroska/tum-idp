#!/usr/bin/env python3
"""
R2 Benchmark Visualization Script

This script analyzes Parquet files generated by the R2 benchmark and creates
various visualizations to understand performance characteristics.
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import argparse
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Set style for better plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

class R2BenchmarkVisualizer:
    def __init__(self, parquet_file):
        """Initialize the visualizer with a Parquet file."""
        self.parquet_file = parquet_file
        self.df = None
        self.load_data()
    
    def load_data(self):
        """Load data from Parquet file."""
        try:
            self.df = pd.read_parquet(self.parquet_file)
            print(f"Loaded {len(self.df)} records from {self.parquet_file}")
            print(f"Time range: {self.df['ts'].min()} to {self.df['ts'].max()}")
        except Exception as e:
            print(f"Error loading Parquet file: {e}")
            raise
    
    def calculate_throughput(self, window='1S'):
        """Calculate throughput over time."""
        # Group by time window and sum bytes transferred
        throughput_df = self.df.groupby(pd.Grouper(key='ts', freq=window)).agg({
            'bytes': 'sum',
            'latency_ms': ['mean', 'p50', 'p90', 'p95', 'p99'],
            'http_status': 'count'
        }).reset_index()
        
        # Calculate throughput in Mbps
        throughput_df['throughput_mbps'] = (throughput_df['bytes']['sum'] * 8) / (1024 * 1024)
        throughput_df['qps'] = throughput_df['http_status']['count']
        
        return throughput_df
    
    def plot_throughput_timeline(self, output_dir='plots'):
        """Plot throughput timeline."""
        throughput_df = self.calculate_throughput()
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))
        
        # Throughput plot
        ax1.plot(throughput_df['ts'], throughput_df['throughput_mbps'], 
                linewidth=2, color='blue', alpha=0.7)
        ax1.set_ylabel('Throughput (Mbps)')
        ax1.set_title('R2 Benchmark - Throughput Timeline')
        ax1.grid(True, alpha=0.3)
        
        # QPS plot
        ax2.plot(throughput_df['ts'], throughput_df['qps'], 
                linewidth=2, color='green', alpha=0.7)
        ax2.set_ylabel('Requests per Second')
        ax2.set_xlabel('Time')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/throughput_timeline.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"Throughput timeline saved to {output_dir}/throughput_timeline.png")
    
    def plot_latency_cdf(self, output_dir='plots'):
        """Plot latency CDF."""
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # Filter successful requests
        successful_df = self.df[self.df['http_status'] == 200]
        
        if len(successful_df) > 0:
            latencies = successful_df['latency_ms'].values
            latencies_sorted = np.sort(latencies)
            p = np.arange(1, len(latencies_sorted) + 1) / len(latencies_sorted)
            
            ax.plot(latencies_sorted, p, linewidth=2, color='red', alpha=0.8)
            ax.set_xlabel('Latency (ms)')
            ax.set_ylabel('Cumulative Probability')
            ax.set_title('R2 Benchmark - Latency CDF')
            ax.grid(True, alpha=0.3)
            
            # Add percentile markers
            percentiles = [50, 90, 95, 99]
            for pct in percentiles:
                value = np.percentile(latencies, pct)
                ax.axvline(value, color='gray', linestyle='--', alpha=0.7)
                ax.text(value, 0.5, f'P{pct}: {value:.1f}ms', 
                       rotation=90, verticalalignment='center')
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/latency_cdf.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"Latency CDF saved to {output_dir}/latency_cdf.png")
    
    def plot_error_histogram(self, output_dir='plots'):
        """Plot error histogram."""
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # Count HTTP status codes
        status_counts = self.df['http_status'].value_counts().sort_index()
        
        bars = ax.bar(status_counts.index.astype(str), status_counts.values, 
                     color=['green' if x == 200 else 'red' for x in status_counts.index],
                     alpha=0.7)
        
        # Add value labels on bars
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{int(height)}', ha='center', va='bottom')
        
        ax.set_xlabel('HTTP Status Code')
        ax.set_ylabel('Count')
        ax.set_title('R2 Benchmark - HTTP Status Code Distribution')
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/error_histogram.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"Error histogram saved to {output_dir}/error_histogram.png")
    
    def plot_concurrency_heatmap(self, output_dir='plots'):
        """Plot concurrency heatmap."""
        # Calculate throughput by concurrency level
        concurrency_stats = self.df.groupby('concurrency').agg({
            'bytes': 'sum',
            'latency_ms': 'mean',
            'http_status': 'count'
        }).reset_index()
        
        concurrency_stats['throughput_mbps'] = (concurrency_stats['bytes'] * 8) / (1024 * 1024)
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Throughput vs Concurrency
        ax1.plot(concurrency_stats['concurrency'], concurrency_stats['throughput_mbps'],
                marker='o', linewidth=2, markersize=8, color='blue', alpha=0.7)
        ax1.set_xlabel('Concurrency')
        ax1.set_ylabel('Throughput (Mbps)')
        ax1.set_title('Throughput vs Concurrency')
        ax1.grid(True, alpha=0.3)
        
        # Latency vs Concurrency
        ax2.plot(concurrency_stats['concurrency'], concurrency_stats['latency_ms'],
                marker='s', linewidth=2, markersize=8, color='red', alpha=0.7)
        ax2.set_xlabel('Concurrency')
        ax2.set_ylabel('Average Latency (ms)')
        ax2.set_title('Latency vs Concurrency')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/concurrency_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        print(f"Concurrency analysis saved to {output_dir}/concurrency_analysis.png")
    
    def plot_system_health(self, output_dir='plots'):
        """Plot system health metrics if available."""
        # Check if we have system metrics
        if 'link_util_pct' in self.df.columns and 'tcp_retx' in self.df.columns:
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))
            
            # Link utilization over time
            link_util = self.df.groupby(pd.Grouper(key='ts', freq='10S'))['link_util_pct'].mean()
            ax1.plot(link_util.index, link_util.values, linewidth=2, color='blue', alpha=0.7)
            ax1.set_ylabel('Link Utilization (%)')
            ax1.set_title('Network Link Utilization Over Time')
            ax1.grid(True, alpha=0.3)
            
            # TCP retransmits over time
            tcp_retx = self.df.groupby(pd.Grouper(key='ts', freq='10S'))['tcp_retx'].mean()
            ax2.plot(tcp_retx.index, tcp_retx.values, linewidth=2, color='red', alpha=0.7)
            ax2.set_ylabel('TCP Retransmits')
            ax2.set_xlabel('Time')
            ax2.set_title('TCP Retransmits Over Time')
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(f'{output_dir}/system_health.png', dpi=300, bbox_inches='tight')
            plt.close()
            print(f"System health plot saved to {output_dir}/system_health.png")
        else:
            print("System health metrics not available in the data")
    
    def generate_summary_report(self, output_dir='plots'):
        """Generate a summary report."""
        # Calculate overall statistics
        total_requests = len(self.df)
        successful_requests = len(self.df[self.df['http_status'] == 200])
        error_rate = (total_requests - successful_requests) / total_requests * 100
        
        total_bytes = self.df['bytes'].sum()
        total_duration = (self.df['ts'].max() - self.df['ts'].min()).total_seconds()
        avg_throughput = (total_bytes * 8) / (1024 * 1024 * total_duration)  # Mbps
        
        latency_stats = self.df[self.df['http_status'] == 200]['latency_ms'].describe()
        
        # Create summary report
        report = f"""
R2 Benchmark Summary Report
===========================

Test Configuration:
- Total Duration: {total_duration:.1f} seconds
- Object Size: {self.df['range_len'].iloc[0] / (1024*1024):.1f} MB per request
- Instance Type: {self.df['instance_type'].iloc[0]}

Results:
- Total Requests: {total_requests:,}
- Successful Requests: {successful_requests:,}
- Error Rate: {error_rate:.2f}%
- Total Data Transferred: {total_bytes / (1024*1024*1024):.2f} GB
- Average Throughput: {avg_throughput:.2f} Mbps

Latency Statistics (successful requests):
- Mean: {latency_stats['mean']:.2f} ms
- Median (P50): {latency_stats['50%']:.2f} ms
- P90: {latency_stats['mean']:.2f} ms (approximate)
- P95: {latency_stats['mean']:.2f} ms (approximate)
- P99: {latency_stats['mean']:.2f} ms (approximate)

Concurrency Levels Tested:
{self.df['concurrency'].unique()}
"""
        
        # Save report
        with open(f'{output_dir}/summary_report.txt', 'w') as f:
            f.write(report)
        
        print(f"Summary report saved to {output_dir}/summary_report.txt")
        print(report)
    
    def create_all_plots(self, output_dir='plots'):
        """Create all visualization plots."""
        # Create output directory
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        print("Creating visualizations...")
        
        # Generate all plots
        self.plot_throughput_timeline(output_dir)
        self.plot_latency_cdf(output_dir)
        self.plot_error_histogram(output_dir)
        self.plot_concurrency_heatmap(output_dir)
        self.plot_system_health(output_dir)
        self.generate_summary_report(output_dir)
        
        print(f"All visualizations saved to {output_dir}/")

def main():
    parser = argparse.ArgumentParser(description='R2 Benchmark Visualization Tool')
    parser.add_argument('parquet_file', help='Path to the Parquet file with benchmark results')
    parser.add_argument('--output-dir', default='plots', help='Output directory for plots (default: plots)')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.parquet_file):
        print(f"Error: Parquet file {args.parquet_file} not found")
        return
    
    # Create visualizer and generate plots
    visualizer = R2BenchmarkVisualizer(args.parquet_file)
    visualizer.create_all_plots(args.output_dir)

if __name__ == '__main__':
    main()
